{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h3PfIqD_ijmx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium# > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tM0HdqM9Uo7v"
   },
   "source": [
    "In this Notebook, we'll implement an agent that plays several matrix (normal-form) games against opponents that have fixed strategies. The games are a.o. the Prisoner's Dilemma, the Chicken game, and the Stag-Hunt game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s-Bl6utuUp7W"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gymnasium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spaces\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkUJNWZdU-Gy"
   },
   "outputs": [],
   "source": [
    "class Qlearner:\n",
    "    \"\"\"A Q-learning agent\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_size,\n",
    "        state_size,\n",
    "        learning_rate=0.0,\n",
    "        gamma=0.95,\n",
    "        epsilon=1.0,\n",
    "        epsilon_min=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "    ):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "        # initialize the Q-table:\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "        # define learning rate:\n",
    "        if learning_rate == 0.0:\n",
    "            self.dynamic_lr = True\n",
    "            self.action_counter = np.zeros((self.state_size, self.action_size))\n",
    "        else:\n",
    "            self.dynamic_lr = False\n",
    "            self.learning_rate = learning_rate\n",
    "\n",
    "        # discount factor:\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Exploration parameters:\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # tracking rewards/progress:\n",
    "        self.rewards_this_episode = []  # during an episode, save every time step's reward\n",
    "        self.episode_total_rewards = []  # each episode, sum the rewards, possibly with a discount factor\n",
    "        self.average_episode_total_rewards = []  # the average (discounted) episode reward to indicate progress\n",
    "\n",
    "        self.state_history = []\n",
    "        self.action_history = []\n",
    "\n",
    "    def reset_agent(self):\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "    def select_greedy(self, state):\n",
    "        # np.argmax(self.qtable[state]) will select first entry if two or more Q-values are equal, but we want true randomness:\n",
    "        return np.random.choice(np.flatnonzero(np.isclose(self.qtable[state], self.qtable[state].max())))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            action = self.select_greedy(state)\n",
    "        self.state_history.append(state)\n",
    "        self.action_history.append(action)\n",
    "        return action\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "\n",
    "    def update(self, state, action, new_state, reward, done, update_epsilon=True):\n",
    "        if not self.dynamic_lr:\n",
    "            lr = self.learning_rate\n",
    "        else:\n",
    "            self.action_counter[state, action] += 1\n",
    "            lr = 1 / self.action_counter[state, action]\n",
    "\n",
    "        # Q(s,a) <-- Q(s,a) + learning_rate [R + gamma * max_a' Q(s',a') - Q(s,a)]\n",
    "        self.qtable[state, action] += lr * (reward + (not done) * self.gamma * np.max(self.qtable[new_state]) - self.qtable[state, action])\n",
    "\n",
    "        self.rewards_this_episode.append(reward)\n",
    "\n",
    "        if done:\n",
    "            # track total reward:\n",
    "            episode_reward = self._calculate_episode_reward(self.rewards_this_episode, discount=False)\n",
    "            self.episode_total_rewards.append(episode_reward)\n",
    "\n",
    "            k = len(self.average_episode_total_rewards) + 1  # amount of episodes that have passed\n",
    "            self._calculate_average_episode_reward(k, episode_reward)\n",
    "\n",
    "            if update_epsilon:\n",
    "                self.update_epsilon()\n",
    "\n",
    "            # reset the rewards for the next episode:\n",
    "            self.rewards_this_episode = []\n",
    "\n",
    "    def _calculate_episode_reward(self, rewards_this_episode, discount=False):\n",
    "        if discount:\n",
    "            return sum([self.gamma**i * reward for i, reward in enumerate(rewards_this_episode)])\n",
    "        return sum(rewards_this_episode)\n",
    "\n",
    "    def _calculate_average_episode_reward(self, k, episode_reward):\n",
    "        if k > 1:  # running average is more efficient:\n",
    "            average_episode_reward = (1 - 1 / k) * self.average_episode_total_rewards[-1] + episode_reward / k\n",
    "        else:\n",
    "            average_episode_reward = episode_reward\n",
    "        self.average_episode_total_rewards.append(average_episode_reward)\n",
    "\n",
    "    def print_rewards(self, episode, print_epsilon=True, print_q_table=True):\n",
    "        # print(\"Episode \", episode + 1)\n",
    "        print(\"Total (discounted) reward of this episode: \", self.episode_total_rewards[episode])\n",
    "        print(\"Average total reward over all episodes until now: \", self.average_episode_total_rewards[-1])\n",
    "\n",
    "        print(\"Epsilon:\", self.epsilon) if print_epsilon else None\n",
    "        print(\"Q-table: \", self.qtable) if print_q_table else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjx2_q0fdDdt"
   },
   "outputs": [],
   "source": [
    "class StaticNormalFormGame(gym.Env):\n",
    "    def __init__(self, reward_matrix, opponent_strategy):\n",
    "        super(StaticNormalFormGame, self).__init__()\n",
    "\n",
    "        self.opponent_strategy = opponent_strategy\n",
    "        self.agent_rewards = []\n",
    "        self.opponent_rewards = []\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Discrete(1)  # Stateless\n",
    "\n",
    "        self.reward_matrix = reward_matrix\n",
    "        self.prev_action = None\n",
    "\n",
    "    def reset(self):\n",
    "        info = {}\n",
    "        return 0, info\n",
    "\n",
    "    def step(self, action):\n",
    "        if action not in [0, 1]:\n",
    "            raise ValueError(\"Invalid action. Must be 0 or 1.\")\n",
    "\n",
    "        opponent_action = self.opponent_strategy(self.prev_action)\n",
    "\n",
    "        # Fetch rewards from matrix\n",
    "        agent_reward, opponent_reward = self.reward_matrix[action, opponent_action]\n",
    "\n",
    "        # Info dictionary containing opponent's reward and action\n",
    "        info = {'opponent_reward': opponent_reward, 'opponent_action': opponent_action}\n",
    "\n",
    "        # Update prev_action for tit-for-tat or other strategies\n",
    "        self.prev_action = action\n",
    "\n",
    "        # Single-state game, so new state is still 0, and the episode is terminated\n",
    "        new_state = 0\n",
    "        terminated = True\n",
    "        truncated = False\n",
    "        return new_state, agent_reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHreNkQpdgCC",
    "outputId": "73b1cc86-d53f-4792-ccf0-1022de7ae224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total (discounted) reward of this episode:  2\n",
      "Average total reward over all episodes until now:  0.8125000000000007\n",
      "Epsilon: 0.017950553275045134\n",
      "Q-table:  [[1.10563018 0.44092966]]\n"
     ]
    }
   ],
   "source": [
    "# Prisoner's Dilemma\n",
    "# Players can either Cooperate (C) or Defect (D).\n",
    "# If both cooperate, both get a moderate reward (3,3).\n",
    "# If one defects and the other cooperates, the defector gets a high reward (5,0 or 0,5).\n",
    "# Mutual defection gives both a low reward (1,1).\n",
    "PD_MATRIX = np.array([[(3, 3), (0, 5)],\n",
    "                      [(5, 0), (1, 1)]])\n",
    "\n",
    "# Stag Hunt\n",
    "# Both players benefit the most if they both hunt the stag (4,4).\n",
    "# If one hunts a rabbit while the other hunts a stag, the rabbit hunter gets a small reward, and the stag hunter gets nothing (3,0 or 0,3).\n",
    "# Both hunting rabbits gives a smaller but safe reward (2,2).\n",
    "STAG_HUNT_MATRIX = np.array([[(4, 4), (0, 3)],\n",
    "                             [(3, 0), (2, 2)]])\n",
    "\n",
    "# Chicken (Hawk-Dove game)\n",
    "# Both players get a moderate reward if they both swerve (2,2).\n",
    "# If one swerves while the other goes straight, the straight driver gets a big reward, while the swerver gets nothing (0,3 or 3,0).\n",
    "# If both go straight, they crash and both get negative rewards (-1, -1).\n",
    "CHICKEN_MATRIX = np.array([[(2, 2), (0, 3)],\n",
    "                           [(3, 0), (-1, -1)]])\n",
    "\n",
    "# Toin-Coss (Matching Pennies)\n",
    "# Each player chooses heads (C) or tails (D).\n",
    "# If both choose heads, both win (1,1). If they mismatch, one wins, and the other loses (1,0 or 0,1).\n",
    "# If both pick tails, both get nothing (0,0).\n",
    "TOIN_COSS_MATRIX = np.array([[(1, 1), (0, 1)],\n",
    "                             [(1, 0), (0, 0)]])\n",
    "\n",
    "# Battle of the Sexes\n",
    "# Players try to coordinate on an activity (opera or football).\n",
    "# One player prefers opera (2,1), and the other prefers football (1,2).\n",
    "# If they fail to coordinate, both get zero (0,0).\n",
    "BATTLE_OF_SEXES_MATRIX = np.array([[(2, 1), (0, 0)],\n",
    "                                   [(0, 0), (1, 2)]])\n",
    "\n",
    "# Matching Pennies\n",
    "# A zero-sum game where the players choose heads (H) or tails (T).\n",
    "# One player wins if the choices match, and the other wins if they differ.\n",
    "# If both pick heads or both pick tails, Player 1 wins (1,-1).\n",
    "# If one picks heads and the other tails, Player 2 wins (-1, 1).\n",
    "MATCHING_PENNIES_MATRIX = np.array([[(1, -1), (-1, 1)],\n",
    "                                    [(-1, 1), (1, -1)]])\n",
    "\n",
    "\n",
    "# Opponent Strategies\n",
    "def cooperate(_):\n",
    "    return 0\n",
    "\n",
    "def defect(_):\n",
    "    return 1\n",
    "\n",
    "def tit_for_tat(prev_action):\n",
    "    if prev_action is None:\n",
    "        return 0\n",
    "    return prev_action\n",
    "\n",
    "def mixed(_):\n",
    "    return np.random.choice([0, 1], p=[0.4, 0.6])  # Example mixed strategy with probabilities\n",
    "\n",
    "\n",
    "# define the environment:\n",
    "reward_matrix = BATTLE_OF_SEXES_MATRIX\n",
    "opponent_strategy = mixed\n",
    "env = StaticNormalFormGame(reward_matrix=reward_matrix, opponent_strategy=opponent_strategy)\n",
    "\n",
    "# define our agent that will play against the opponents\n",
    "learning_rate = 0.1\n",
    "gamma = 0.95  # will not matter here, non-episodic games\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.99\n",
    "qlearner = Qlearner(\n",
    "    action_size=env.action_space.n,\n",
    "    state_size=env.observation_space.n,\n",
    "    learning_rate=learning_rate,\n",
    "    gamma=gamma,\n",
    "    epsilon=epsilon,\n",
    "    epsilon_min=epsilon_min,\n",
    "    epsilon_decay=epsilon_decay\n",
    "  )\n",
    "\n",
    "num_episodes = 400\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not terminated:\n",
    "        action = qlearner.select_action(state)\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        qlearner.update(state, action, new_state, reward, terminated)\n",
    "        state = new_state  # just stays 0, the environment is stateless\n",
    "qlearner.print_rewards(num_episodes-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIoDSWtcitdr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
