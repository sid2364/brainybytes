{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aQKQMJTJBPH"
   },
   "source": [
    "# Q-learning for FrozenLake 4x4\n",
    "\n",
    "In this Notebook, we'll implement an agent that plays <b>FrozenLake.</b>\n",
    "\n",
    "![alt text](http://simoninithomas.com/drlc/Qlearning/frozenlake4x4.png)\n",
    "\n",
    "The goal of this game is to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H). There are four directions you can move to, however, the ice is slippery, so you won't always move in the direction you intend to; this is a stochastic environment.\n",
    "\n",
    "We'll use the implementation of the Frozen Lake environment from OpenAI's gymnasium, where it is described as follows (see also: [link](https://gymnasium.farama.org/environments/toy_text/frozen_lake/))\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the\n",
    "    park when you made a wild throw that left the frisbee out in the middle of\n",
    "    the lake. The water is mostly frozen, but there are a few holes where the\n",
    "    ice has melted. If you step into one of those holes, you'll fall into the\n",
    "    freezing water. At this time, there's an international frisbee shortage, so\n",
    "    it's absolutely imperative that you navigate across the lake and retrieve\n",
    "    the disc. However, the ice is slippery, so you won't always move in the\n",
    "    direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "```\n",
    "\n",
    "Our agent does not know this stochasticity (it needs to discover this), but the environment dynamics are modeled like this: if you choose a direction (i.e., action), you have a 1/3 chance of actually going in that direction, but also a 1/3 chance of taking one of the perpendicular actions. Remember that at the start, an agent doesn't know what an action means. Its policy only outputs a number between [0, 3], and each of those numbers corresponds to something that happens in the environment (here: moving the agent in the grid) and which the agent has to learn to map. In our case, the environment maps those integers to actions like this:\n",
    "\n",
    "```\n",
    "  0: LEFT\n",
    "  1: DOWN\n",
    "  2: RIGHT\n",
    "  3: UP\n",
    "```\n",
    "\n",
    "To give you an example, if your agent selects action 0, the environment implements action 'LEFT', but since the environment is also stochastic, there is a 33% chance of actually going left, a 33% chance of going up, and a 33% chance of going down. There is 0% chance of going in the reverse direction, in this case 'RIGHT'. If an attempt at moving encounters a wall, there is no movement.\n",
    "\n",
    "**Once again, your agent does not have this kind of information at the start, I only provide this here to give you some intuition about this specific problem. The agent needs to learn all of this by scratch by probing the environment by taking actions and using the rewards and observations it gets back.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54AIvDov_7aa"
   },
   "source": [
    "## Step -1: Install the dependencies on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxxpHDIs_lvg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in ./env/lib/python3.12/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./env/lib/python3.12/site-packages (from gymnasium) (2.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./env/lib/python3.12/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./env/lib/python3.12/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./env/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.7/163.7 kB\u001b[0m \u001b[31m742.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in ./env/lib/python3.12/site-packages (from matplotlib) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./env/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m964.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (320 kB)\n",
      "\u001b[?25l   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/320.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9qH33L_QoBk"
   },
   "source": [
    "## Step 0: Import the dependencies 📚\n",
    "We use 2 libraries:\n",
    "- `Numpy` for our Q-table\n",
    "- `OpenAI Gymnasium` for our FrozenLake Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oU8zRXv8QHlm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fz-X3HTQueX"
   },
   "source": [
    "## Step 1: Create the environment 🎮\n",
    "Here we'll create the FrozenLake 4x4 environment by simply loading it from Gym.\n",
    "\n",
    "OpenAI Gym is a library from OpenAI containing many environments that researchers often use to test their reinforcement learning algorithms on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mh9jBR_cQ5_a"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEtXMldxQ7uw"
   },
   "source": [
    "## Step 2: Create the Q-table and initialize it\n",
    "- To create our initial Q-table, we need to know how much states (rows) and  actions (columns) there are.\n",
    "- OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uc0xDVd_Q-C8",
    "outputId": "d874bebc-c601-4ceb-c79b-17fc6dd5878e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of actions:  4\n",
      "Amount of states:  16\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "print(\"Amount of actions: \", action_size)\n",
    "state_size = env.observation_space.n\n",
    "print(\"Amount of states: \", state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17XBPCecAsZG",
    "outputId": "6743cada-2879-40ce-a708-9a943b545196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Create our Q-table with state_size rows and action_size columns (16x4)\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEGeWKKsAu7X"
   },
   "source": [
    "## Step 3: Set the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FJhPxx7UAunE"
   },
   "outputs": [],
   "source": [
    "total_episodes = 20000       # Total episodes\n",
    "learning_rate = 0.7          # Learning rate\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discount rate\n",
    "# gamma = 1.\n",
    "\n",
    "# Exploration parameters\n",
    "exploration = 1.0            # Exploration probability\n",
    "exploration_min = 0.01       # Minimum exploration probability\n",
    "exploration_decay = 0.995    # Exponential decay rate for the exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcdaN_DbA3ES"
   },
   "source": [
    "## Step 4: The Q-learning algorithm\n",
    "The Q-learning algorithm goes like this:   \n",
    "<br>\n",
    "**Initialize** all Q-values _Q(s,a)_ arbitrarily (often simply **to zero**)    \n",
    "Repeat **for each episode**:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Reset state _s_ to the starting state  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Repeat **for each step in the episode** until terminal state:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Pick action _a_** for state _s_ according to the current policy (argmax_a over all _Q(s,a)_ values) **or** a random action with small probability  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Execute action _a_ and **observe reward _R_ and next state _s'_**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Improve _Q(s,a)_** by using the received reward and the maximum Q-value of the next step:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_Q(s,a)_&nbsp;&nbsp; <---- &nbsp;&nbsp;_Q(s,a) + learning_rate * (R + gamma * max_a'(Q(s',a')) - Q(s,a))_  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set state _s_ = _s'_ to go to the next step  \n",
    "<br>\n",
    "\n",
    "\n",
    "In the next part, we'll go over the implementation of the Q-learning algorithm, and train an agent to navigate itself through the slippery FrozenLake environment.\n",
    "\n",
    "We'll start off with a completely random agent. This random agent will explore its environment, and learn from its interactions with it. After a while, the agent learns that certain actions in certain states are better than others. The exploration rate decreases over time, so our agent gradually starts using its learned policy more often, until it converges to a policy that on average more or less maximizes the reward in the environment. I stress 'on average', since the environment is very stochastic, and the slippery nature of the floor often makes sure that no matter what the policy of our agent is, it will still end up in a hole in the ice from time to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5Dqqo_8LA5De",
    "outputId": "ce92dd2b-b6d4-436e-c82f-0543080aa446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total (discounted) reward for episode 1 :  0.0\n",
      "Average total reward over episodes until now:  0.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage total reward over episodes until now: \u001b[39m\u001b[38;5;124m\"\u001b[39m, average_episode_rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     47\u001b[0m       \u001b[38;5;66;03m# Render the last frame of our frozenlake environment:\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m       \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(env\u001b[38;5;241m.\u001b[39mrender())\n\u001b[1;32m     49\u001b[0m       plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAverage total reward over all episodes: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(episode_rewards) \u001b[38;5;241m/\u001b[39m total_episodes)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "average_episode_rewards = []\n",
    "\n",
    "# We learn for a predefined amount of episodes:\n",
    "for episode in range(total_episodes):\n",
    "\n",
    "    # Reset the environment:\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps): # until done = True or it reaches max_steps\n",
    "\n",
    "        # There is always a chance that we take a random action for exploration:\n",
    "        if np.random.rand() < exploration:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Else, simply pick the best action according to your current best policy:\n",
    "        else:\n",
    "            action = np.argmax(qtable[state])\n",
    "\n",
    "        # Give this action to your environment, and observe the new state your agent is in and the reward it got:\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + learning_rate [R + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n",
    "\n",
    "        # We add the (discounted) reward to the total reward\n",
    "        total_reward += gamma**step * reward\n",
    "\n",
    "        # Set our state to the new_state:\n",
    "        state = new_state\n",
    "\n",
    "        # If we reached the goal or we fell into a hole:\n",
    "        if terminated == True:\n",
    "            break\n",
    "\n",
    "    # Decay exploration per episode:\n",
    "    exploration *= exploration_decay\n",
    "    exploration = max(exploration_min, exploration)\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    average_episode_rewards.append(sum(episode_rewards) / (episode + 1))\n",
    "\n",
    "    if episode % 1000 == 0:\n",
    "      print(\"\\nTotal (discounted) reward for episode\", episode + 1, \": \", total_reward)\n",
    "      print(\"Average total reward over episodes until now: \", average_episode_rewards[-1])\n",
    "      # Render the last frame of our frozenlake environment:\n",
    "      plt.imshow(env.render())\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\\nAverage total reward over all episodes: \", sum(episode_rewards) / total_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lPm2UcDwpAF"
   },
   "source": [
    "We can plot the average total reward over all episodes. We see that after a while, the average reward stops improving, meaning we've hit a good policy. To make sure that we've got the best one, we should actually play around with the algorithm's parameters (like the exploration decay etc.), but let's keep it simple and trust that our agent has found the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "f5_zYOZ5jIm1",
    "outputId": "8c27a984-3f1c-466f-9ff6-883c4401a6f8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iterations = range(0, total_episodes, 1)\n",
    "plt.plot(iterations, average_episode_rewards)\n",
    "plt.ylabel('Average total reward over all episodes')\n",
    "plt.xlabel('Episode')\n",
    "# plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOQaHObPquNK"
   },
   "source": [
    "The (by now optimal) policy is created by acting greedily (argmax) over the following Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6F6sK1mPoCHb",
    "outputId": "ed6c636e-ea4c-40dc-ee0d-0c314eca6813"
   },
   "outputs": [],
   "source": [
    "print(\"Q-table for the optimal policy:\\n\")\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbitR6_Xrccq"
   },
   "source": [
    "The agent doesn't actually know the concept of left, right, up or down. It simply outputs an action, which is an integer between [0, 3]. The environment understands this number, and changes the state according to this action and the environment dynamics (the random slipperyness of the frozen lake). To see what the policy actually is in human language, we'll change it according to the [translation key](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py) from OpenAI:\n",
    "\n",
    "```\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UofksKtksSf-",
    "outputId": "7de47f90-dd94-4f56-9a31-30de627c7820"
   },
   "outputs": [],
   "source": [
    "translation_key = {0: \"LEFT\", 1: \"DOWN\", 2: \"RIGHT\", 3: \"UP\"}\n",
    "\n",
    "policy = np.argmax(qtable, axis=1)\n",
    "print(\"Numerical policy:\\n\\n\", np.resize(policy, (4,4)))\n",
    "human_policy = [translation_key[key] for key in policy]\n",
    "print(\"\\nHuman policy:\\n\\n\", np.resize(human_policy, (4,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCUdoLdDuSDl"
   },
   "source": [
    "As we can see, for the FrozenLake environment\n",
    "\n",
    "```\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "```\n",
    "there are some (at first) seemingly odd policy choices. For example, the first action is always left, instead of e.g. down. This is because the agent learns that if it goes down directly, it might end up somewhere it didn't intend to (slippery floor). If it keeps selecting 'left', however, the only place it will eventually end up is 'down', which is what it really wants.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5czk9qTBQIU"
   },
   "source": [
    "## Step 5: Use our Q-table to play FrozenLake 👾\n",
    "After 20 000 episodes, our Q-table converges to an optimal one, which we can use to play Frozen Lake with an optimal policy, thereby on average maximizing the total reward in an episode!  \n",
    "Let's play an optimal game and really render the actions the agent picks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vk1DUbPON6e",
    "outputId": "1b9b1232-e13c-4612-ff36-793b10511c75"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y xvfb python-opengl ffmpeg\n",
    "!pip install imageio[ffmpeg]\n",
    "!pip install pyglet==1.5.11\n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fd-69I-SA9K",
    "outputId": "9ffc421e-3336-4eae-f5b7-97ff77cc8cd5"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y xvfb > /dev/null 2>&1\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Start virtual display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bt8UsREaBNkJ",
    "outputId": "da279f6c-1eda-4e17-aa37-63853ef20824"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "frames = []\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\n",
    "state, _ = env.reset()\n",
    "frames.append(env.render())\n",
    "for step in range(max_steps):\n",
    "  action = np.argmax(qtable[state])\n",
    "  new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "  frames.append(env.render())\n",
    "  if terminated:\n",
    "    break\n",
    "  state = new_state\n",
    "\n",
    "if new_state == 15:\n",
    "  print(\"\\nReached the goal 🏆\")\n",
    "else:\n",
    "  print(\"\\nFell into a hole ☠️\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9amIABjOqYx"
   },
   "outputs": [],
   "source": [
    "video_filename = 'cartpole-video.mp4'\n",
    "imageio.mimsave(video_filename, [np.array(frame) for frame in frames], fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "MUhLz66uSg4D",
    "outputId": "02ede5d4-1102-4cf4-d1bb-f0cd85b0c2ea"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "# Display the video\n",
    "Video(video_filename, embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhJxJfjwS-2B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
